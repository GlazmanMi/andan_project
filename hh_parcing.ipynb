{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API hh.ru нужно две недели для предоставления токена доступа и вообще она не очень адаптирована для парсинга вакансий (больше под работодателей), так что мы не ищем легких путей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fake_useragent\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [22:24,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n",
      "e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900it [52:10,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "#получаем ссылки на нужные вакансии\n",
    "def get_links(text):\n",
    "    #используем useragent, чтобы больше походить на человека\n",
    "    ua = fake_useragent.UserAgent()\n",
    "    #ссылка на страницу для парсинга (с номером страницы)\n",
    "    res = requests.get(\n",
    "        url = f\"https://hh.ru/vacancies/{text}?page=1\",\n",
    "        headers = {\"user-agent\":ua.random}\n",
    "    )\n",
    "    #условие того, что сервер вернул данные  \n",
    "    if res.status_code != 200:  return\n",
    "    #передаем содержание страницы (дерево)\n",
    "    soup = BeautifulSoup(res.content, \"html\")\n",
    "    #считаем количество страниц с вакансиями администраторов\n",
    "    try:\n",
    "        finded_div = soup.find(\"div\", attrs = {\"class\":\"pager\"}) \n",
    "        cnt_pages = int(finded_div.find_all(\"span\", recursive = False)[-1].find(\"a\").find(\"span\").text)\n",
    "    except:\n",
    "        return\n",
    "    #идем по всем страницам\n",
    "    for page in range(cnt_pages):\n",
    "        #собираем вакансии со страницы\n",
    "        try:\n",
    "            res = requests.get(\n",
    "            url=f\"https://hh.ru/vacancies/{text}?page={page}\",\n",
    "            headers={\"user-agent\":ua.random}\n",
    "            )\n",
    "            if res.status_code != 200:\n",
    "                continue\n",
    "            else:\n",
    "            #собираем ссылки на все вакансии с конкретной страницы\n",
    "                soup = BeautifulSoup(res.content, \"html\")\n",
    "                for i in soup.find_all('a', attrs = {\"class\": \"bloko-link\", \"target\": \"_blank\"}):\n",
    "                    l = f\"{i.attrs['href'].split('?')[0]}\" #создаем ссылки на вакансии\n",
    "                    if 'vacancy' in l: #помимимо нужных ссылок у меня парсились ссылки на фидбэк и еще несколько странных, не хочется их возвращать\n",
    "                        yield l \n",
    "        except Exception as e: print(f'e')\n",
    "        time.sleep(1)  #пытаемся еще больше быть похожим на человека\n",
    "#получаем страницу вакансии по ссылке и парсим с нее нужную нам информацию\n",
    "def get_vacancy(link):\n",
    "    #используем useragent, чтобы больше походить на человека\n",
    "    ua = fake_useragent.UserAgent()\n",
    "    #получаем страницу конвертной вакансии\n",
    "    data = requests.get(\n",
    "        url = link,\n",
    "        headers = {\"user-agent\":ua.random}\n",
    "    )\n",
    "    #получилось ли спарсить информацию со страницы\n",
    "    if data.status_code != 200: return\n",
    "    soup = BeautifulSoup(data.content, 'html')\n",
    "    #получаем названия вакансий\n",
    "    try:\n",
    "        name = soup.find(attrs = {\"class\":\"bloko-header-section-1\"}).text\n",
    "    except:\n",
    "        name = \"\"\n",
    "    #получаем зарплату указанную в вакансии\n",
    "    try:\n",
    "        #зарплата парситься в неудобном формате, поэтому используем replace, чтобы потом было проще обрабатывать данные\n",
    "        salary = soup.find(attrs = {\"class\":\"bloko-header-section-2 bloko-header-section-2_lite\"}).text.replace('\\xa0', '') \n",
    "    except:\n",
    "        salary = \"\"\n",
    "    #получаем набор ключевых навыков (на hh.ru есть соответствующее поле)\n",
    "    try:\n",
    "        #сначала находим на странице необходимое поле, потом все навыки из него записываем в массив skills\n",
    "        skills = [skill.text for skill in soup.find(attrs = {\"class\":\"bloko-tag-list\"}).find_all(attrs = {\"class\":\"bloko-tag bloko-tag_inline\"})]\n",
    "    except:\n",
    "        skills = []\n",
    "    #создаем обьект класса json - вакансия, с интересующей нас информацией: название, з/п, ключевые навыки\n",
    "    vacancy = {\n",
    "        \"name\": name,\n",
    "        \"salary\":salary,\n",
    "        \"skills\":skills\n",
    "    }\n",
    "    return vacancy\n",
    "\n",
    "#создаем массив с данными, который будем заполнять\n",
    "data = []\n",
    "#выводим спаршенные данные в json файл\n",
    "for link in tqdm(get_links(\"administrator\")):\n",
    "    data.append(get_vacancy(link))\n",
    "    #примерно раз в секунду будет парситься новая вакансия: очень долго (около часа),\n",
    "    # но зато мы теперь очень похожи на человека, скролящего вакансии администраторов\n",
    "    time.sleep(1)\n",
    "    with open ('hh_data.json', 'w', encoding = 'utf-8') as outfile:\n",
    "        json.dump(data, outfile, indent=4, ensure_ascii = False) #не хотим, чтобы русские буквы на что-то заменялись"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсер падал только два раза (из-за того, что я отклюался от VPN). При этом количество вакансий, указанных на hh не совпадает с количеством вакансий, который находятся на 40 открытых страницах"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
